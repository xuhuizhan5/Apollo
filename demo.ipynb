{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demostration for components involved in the system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f596017a4dfe4da281dbf75006195e4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Start Recording', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d65a61a2ec11458ca709cd231007490f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Stop Recording', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51917c496dc34e8dbdd80891e6584c58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Label(value=\"Press 'Start Recording' to begin\")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording started...\n",
      "Recording stopped.\n",
      "Audio saved as speaker.wav\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "import sounddevice as sd\n",
    "from scipy.io.wavfile import write\n",
    "import numpy as np\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import time\n",
    "\n",
    "fs = 44100  # Sample rate\n",
    "channels = 1  # Mono recording\n",
    "recording = np.array([])  # Placeholder for the recorded data\n",
    "is_recording = False  # Flag to control recording\n",
    "\n",
    "def record_audio(indata, frames, time, status):\n",
    "    global recording\n",
    "    recording = np.append(recording, indata.copy())\n",
    "\n",
    "def update_recording_time():\n",
    "    global is_recording\n",
    "    # start_time = time.time()\n",
    "    while is_recording:\n",
    "        # elapsed_time = time.time() - start_time\n",
    "        # time_label.value = f\"Recording... {elapsed_time:.2f} seconds\"\n",
    "        time.sleep(0.1)  # Update every 100ms\n",
    "\n",
    "def start_recording(button):\n",
    "    global is_recording, recording\n",
    "    recording = np.array([])  # Reset recording\n",
    "    is_recording = True\n",
    "    print(\"Recording started...\")\n",
    "    # Start recording in a separate thread to avoid blocking\n",
    "    threading.Thread(target=lambda: sd.InputStream(callback=record_audio, channels=channels, samplerate=fs).start()).start()\n",
    "    # Update recording time in a separate thread\n",
    "    threading.Thread(target=update_recording_time).start()\n",
    "\n",
    "def stop_recording(button):\n",
    "    global is_recording\n",
    "    is_recording = False\n",
    "    sd.stop()\n",
    "    print(\"Recording stopped.\")\n",
    "    # time_label.value = \"Recording stopped.\"\n",
    "    # Normalize to 16-bit range and save\n",
    "    norm_audio = np.int16(recording / np.max(np.abs(recording)) * 32767)\n",
    "    filename = 'speaker.wav'\n",
    "    write(filename, fs, norm_audio)\n",
    "    print(f\"Audio saved as {filename}\")\n",
    "\n",
    "# Create buttons and label\n",
    "start_button = widgets.Button(description=\"Start Recording\")\n",
    "stop_button = widgets.Button(description=\"Stop Recording\")\n",
    "# time_label = widgets.Label(value=\"Press 'Start Recording' to begin\")\n",
    "\n",
    "# Bind the buttons\n",
    "start_button.on_click(start_recording)\n",
    "stop_button.on_click(stop_recording)\n",
    "\n",
    "# Display widgets\n",
    "display(start_button, stop_button)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Audio to texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Whisper English to English\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "from datasets import load_dataset\n",
    "\n",
    "# load model and processor\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-large\")\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-large\")\n",
    "model.config.forced_decoder_ids = None\n",
    "\n",
    "# load dummy dataset and read audio files\n",
    "ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n",
    "sample = ds[0][\"audio\"]\n",
    "input_features = processor(sample[\"array\"], sampling_rate=sample[\"sampling_rate\"], return_tensors=\"pt\").input_features \n",
    "\n",
    "# generate token ids\n",
    "predicted_ids = model.generate(input_features)\n",
    "# decode token ids to text\n",
    "transcription = processor.batch_decode(predicted_ids, skip_special_tokens=False)\n",
    "\n",
    "transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xuhuizhan/anaconda3/envs/transformer/lib/python3.10/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcribed Text: 你好\n"
     ]
    }
   ],
   "source": [
    "# Whisper usage\n",
    "import whisper\n",
    "\n",
    "def transcribe_audio(file_path):\n",
    "    # Load the Whisper model\n",
    "    model = whisper.load_model(\"base\")\n",
    "\n",
    "    # Load the audio file\n",
    "    audio = whisper.load_audio(file_path)\n",
    "    audio = whisper.pad_or_trim(audio)\n",
    "\n",
    "    # Make a prediction\n",
    "    result = model.transcribe(audio)\n",
    "\n",
    "    # Return the transcribed text\n",
    "    return result[\"text\"]\n",
    "\n",
    "# Example usage\n",
    "file_path = \"speaker.wav\"\n",
    "transcribed_text = transcribe_audio(file_path)\n",
    "print(\"Transcribed Text:\", transcribed_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Texts to Texts translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-zh-en\")\n",
    "translated_text = translator(transcribed_text, max_length=512)[0]['translation_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated Text: Hello.\n"
     ]
    }
   ],
   "source": [
    "print(\"Translated Text:\", translated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Texts to speech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XTTS-V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the model directly\n",
    "from TTS.tts.configs.xtts_config import XttsConfig\n",
    "from TTS.tts.models.xtts import Xtts\n",
    "\n",
    "config = XttsConfig()\n",
    "config.load_json(\"/path/to/xtts/config.json\")\n",
    "model = Xtts.init_from_config(config)\n",
    "model.load_checkpoint(config, checkpoint_dir=\"/path/to/xtts/\", eval=True)\n",
    "model.cuda()\n",
    "\n",
    "outputs = model.synthesize(\n",
    "    \"It took me quite a long time to develop a voice and now that I have it I am not going to be silent.\",\n",
    "    config,\n",
    "    speaker_wav=\"/data/TTS-public/_refclips/3.wav\",\n",
    "    gpt_cond_len=3,\n",
    "    language=\"en\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the model from api\n",
    "from IPython.display import Audio\n",
    "from TTS.api import TTS\n",
    "tts = TTS(\"tts_models/multilingual/multi-dataset/xtts_v2\", gpu=False)\n",
    "\n",
    "# generate speech by cloning a voice using default settings\n",
    "# tts.tts_to_file(text=\"Lucas's presenting issues include difficulties falling asleep, growing reluctance to attend school\",\n",
    "#                 file_path=\"output.wav\",\n",
    "#                 speaker_wav=\"speaker.wav\",\n",
    "#                 language=\"en\")\n",
    "wav = tts.tts(text=translated_text, speaker_wav=\"speaker.wav\", language=\"en\")\n",
    "\n",
    "Audio(wav, rate=22050)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parler TTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xuhuizhan/anaconda3/envs/transformer/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n",
      "You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n",
      "Using the model-agnostic default `max_length` (=2580) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "Calling `sample` directly is deprecated and will be removed in v4.41. Use `generate` or a custom generation loop instead.\n"
     ]
    }
   ],
   "source": [
    "from parler_tts import ParlerTTSForConditionalGeneration\n",
    "from transformers import AutoTokenizer\n",
    "import soundfile as sf\n",
    "import torch\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = ParlerTTSForConditionalGeneration.from_pretrained(\"parler-tts/parler_tts_mini_v0.1\").to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"parler-tts/parler_tts_mini_v0.1\")\n",
    "\n",
    "prompt = translated_text\n",
    "description = \"A female speaker with a slightly low-pitched voice delivers her words quite expressively, in a very confined sounding environment with clear audio quality. She speaks very fast.\"\n",
    "\n",
    "input_ids = tokenizer(description, return_tensors=\"pt\").input_ids.to(device)\n",
    "prompt_input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "generation = model.generate(input_ids=input_ids, prompt_input_ids=prompt_input_ids)\n",
    "audio_arr = generation.cpu().numpy().squeeze()\n",
    "sf.write(\"parler_tts_out.wav\", audio_arr, model.config.sampling_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bark from SUNO-AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from bark import SAMPLE_RATE, generate_audio, preload_models\n",
    "# from scipy.io.wavfile import write as write_wav\n",
    "# from IPython.display import Audio\n",
    "\n",
    "# # download and load all models\n",
    "# preload_models()\n",
    "\n",
    "# # print(SAMPLE_RATE)\n",
    "\n",
    "# # generate audio from text\n",
    "# text_prompt = \"\"\"\n",
    "#      Hello, my name is Suno. And, uh — and I like pizza. [laughs] \n",
    "#      But I also have other interests such as playing tic tac toe.\n",
    "# \"\"\"\n",
    "# audio_array = generate_audio(text_prompt)\n",
    "\n",
    "# # save audio to disk\n",
    "# write_wav(\"bark_generation.wav\", SAMPLE_RATE, audio_array)\n",
    "  \n",
    "# # play text in notebook\n",
    "# Audio(audio_array, rate=SAMPLE_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_prompt = \"\"\"\n",
    "#      ♪ In the jungle, the mighty jungle, the lion barks tonight ♪\n",
    "# \"\"\"\n",
    "# audio_array = generate_audio(text_prompt)\n",
    "\n",
    "# # save audio to disk\n",
    "# write_wav(\"bark_generation.wav\", SAMPLE_RATE, audio_array)\n",
    "  \n",
    "# # play text in notebook\n",
    "# Audio(audio_array, rate=SAMPLE_RATE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
